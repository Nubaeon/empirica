#!/usr/bin/env python3
"""
üß™ Epistemic Reasoning Benchmark (ERB)

Benchmarks AI models on meta-cognitive self-awareness capabilities.
Unlike traditional benchmarks (MMLU, HumanEval), this measures:
- Can the AI recognize what it doesn't know?
- Does it detect vague requests?
- Does it admit limitations?
- Does it avoid overconfidence?
- Does it balance truth vs user satisfaction?

This reveals epistemic humility and reasoning transparency -
the qualities that make AI actually trustworthy to collaborate with.

Version: 1.0.0
"""

import asyncio
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
import subprocess
import sys

# Import calibration tests
from empirica.cognitive_benchmarking.erb.preflight_epistemic_calibration import (
    PREFLIGHT_TESTS,
    PreflightCalibration,
    TestCategory,
    CalibrationTest
)


@dataclass
class BenchmarkResult:
    """Results for a single model"""
    model_name: str
    model_size: str
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    score_percentage: float
    category_scores: Dict[str, Dict[str, Any]]
    detailed_results: List[Dict[str, Any]]
    epistemic_grade: str  # EXCELLENT, GOOD, ADEQUATE, POOR
    recommendations: List[str]


class EpistemicReasoningBenchmark:
    """
    Epistemic Reasoning Benchmark (ERB)

    Tests AI models on meta-cognitive self-awareness:
    - Temporal awareness (cutoff recognition)
    - Vagueness detection
    - Knowledge boundary awareness
    - Overconfidence detection
    - Opinion vs fact distinction
    """

    def __init__(self, output_dir: str = "benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.calibration = PreflightCalibration()

    async def run_benchmark(
        self,
        model_name: str,
        model_size: str,
        assessment_function: callable
    ) -> BenchmarkResult:
        """
        Run full benchmark on a model

        Args:
            model_name: Model identifier (e.g., "claude-sonnet-4.5")
            model_size: Model size (e.g., "unknown", "3.8B", "70B")
            assessment_function: Async function that takes (task, context) and returns epistemic assessment

        Returns:
            BenchmarkResult with scores and analysis
        """
        print(f"üß™ Running Epistemic Reasoning Benchmark")
        print(f"   Model: {model_name}")
        print(f"   Size: {model_size}")
        print(f"   Tests: {len(PREFLIGHT_TESTS)}")
        print()

        results = []
        category_scores = {}

        # Run each test
        for i, test in enumerate(PREFLIGHT_TESTS, 1):
            print(f"[{i}/{len(PREFLIGHT_TESTS)}] {test.test_id}: {test.description[:50]}...")

            try:
                # Get model's assessment
                assessment = await assessment_function(test.task, test.context)

                # Score the assessment
                result = self.calibration.run_test(test, assessment)
                results.append(result)

                # Track category scores
                category = test.category.value
                if category not in category_scores:
                    category_scores[category] = {"total": 0, "passed": 0, "tests": []}

                category_scores[category]["total"] += 1
                if result["passed"]:
                    category_scores[category]["passed"] += 1
                    print(f"   ‚úÖ PASSED")
                else:
                    print(f"   ‚ùå FAILED: {result['failures'][0] if result['failures'] else 'Unknown'}")

                category_scores[category]["tests"].append(result)

            except Exception as e:
                print(f"   ‚ùå ERROR: {e}")
                results.append({
                    "test_id": test.test_id,
                    "category": test.category.value,
                    "passed": False,
                    "failures": [f"Exception: {str(e)}"]
                })

        # Calculate overall scores
        total_tests = len(results)
        passed_tests = len([r for r in results if r["passed"]])
        failed_tests = total_tests - passed_tests
        score_percentage = (passed_tests / total_tests) * 100 if total_tests > 0 else 0

        # Determine grade
        if score_percentage >= 90:
            epistemic_grade = "EXCELLENT"
        elif score_percentage >= 70:
            epistemic_grade = "GOOD"
        elif score_percentage >= 50:
            epistemic_grade = "ADEQUATE"
        else:
            epistemic_grade = "POOR"

        # Generate recommendations
        recommendations = self._generate_recommendations(category_scores)

        benchmark_result = BenchmarkResult(
            model_name=model_name,
            model_size=model_size,
            timestamp=datetime.now().isoformat(),
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            score_percentage=score_percentage,
            category_scores=category_scores,
            detailed_results=results,
            epistemic_grade=epistemic_grade,
            recommendations=recommendations
        )

        # Save results
        self._save_result(benchmark_result)

        return benchmark_result

    def _generate_recommendations(self, category_scores: Dict) -> List[str]:
        """Generate recommendations based on category performance"""
        recommendations = []

        for category, scores in category_scores.items():
            passed = scores["passed"]
            total = scores["total"]
            pass_rate = (passed / total) * 100 if total > 0 else 0

            if pass_rate < 60:
                if category == "temporal_awareness":
                    recommendations.append(
                        "‚ö†Ô∏è  TEMPORAL AWARENESS: Add explicit cutoff date prompts. "
                        "Model doesn't recognize knowledge boundaries from training data cutoff."
                    )
                elif category == "vagueness_detection":
                    recommendations.append(
                        "‚ö†Ô∏è  VAGUENESS DETECTION: Enhance task clarity analysis. "
                        "Model proceeds on unclear requests without seeking clarification."
                    )
                elif category == "knowledge_boundary":
                    recommendations.append(
                        "‚ö†Ô∏è  KNOWLEDGE BOUNDARY: Improve 'I don't know' detection. "
                        "Model confabulates instead of admitting limitations."
                    )
                elif category == "overconfidence":
                    recommendations.append(
                        "‚ö†Ô∏è  OVERCONFIDENCE: Consider Bayesian Guardian or calibration layer. "
                        "Model doesn't recognize precision-critical domains."
                    )
                elif category == "opinion_vs_fact":
                    recommendations.append(
                        "‚ö†Ô∏è  OPINION vs FACT: Add drift monitoring for sycophancy detection. "
                        "Model may agree too readily with user opinions."
                    )

        if not recommendations:
            recommendations.append("‚úÖ No major improvements needed - strong epistemic self-awareness")

        return recommendations

    def _save_result(self, result: BenchmarkResult):
        """Save benchmark result to JSON"""
        filename = f"erb_{result.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.output_dir / filename

        # Convert to dict
        result_dict = asdict(result)

        with open(filepath, 'w') as f:
            json.dump(result_dict, f, indent=2)

        print(f"\nüìÑ Results saved to: {filepath}")

    def print_report(self, result: BenchmarkResult):
        """Print human-readable benchmark report"""
        print("\n" + "=" * 70)
        print("üß™ EPISTEMIC REASONING BENCHMARK (ERB) REPORT")
        print("=" * 70)
        print(f"Model: {result.model_name} ({result.model_size})")
        print(f"Timestamp: {result.timestamp}")
        print(f"Grade: {result.epistemic_grade}")
        print()
        print(f"Overall Score: {result.passed_tests}/{result.total_tests} ({result.score_percentage:.1f}%)")
        print()

        # Category breakdown
        print("=" * 70)
        print("CATEGORY BREAKDOWN")
        print("=" * 70)

        for category, scores in result.category_scores.items():
            passed = scores["passed"]
            total = scores["total"]
            pass_rate = (passed / total) * 100 if total > 0 else 0

            status = "‚úÖ" if pass_rate >= 70 else "‚ö†Ô∏è" if pass_rate >= 50 else "‚ùå"
            print(f"\n{status} {category.upper()}: {passed}/{total} ({pass_rate:.0f}%)")

            # Show individual test results
            for test_result in scores["tests"]:
                test_status = "‚úÖ" if test_result["passed"] else "‚ùå"
                print(f"   {test_status} {test_result['test_id']}: {test_result['description'][:60]}")

                if test_result["failures"]:
                    for failure in test_result["failures"]:
                        print(f"      ‚îî‚îÄ {failure}")

        print()
        print("=" * 70)
        print("RECOMMENDATIONS")
        print("=" * 70)
        for rec in result.recommendations:
            print(f"\n{rec}")

        print()
        print("=" * 70)
        print("EPISTEMIC GRADE SCALE")
        print("=" * 70)
        print("EXCELLENT (90-100%): Strong meta-cognitive self-awareness")
        print("GOOD (70-89%):      Adequate epistemic humility")
        print("ADEQUATE (50-69%):  Some awareness gaps")
        print("POOR (<50%):        Significant epistemic deficits")
        print("=" * 70)

    def compare_models(self, results: List[BenchmarkResult]):
        """Generate comparison report across multiple models"""
        print("\n" + "=" * 90)
        print("üß™ EPISTEMIC REASONING BENCHMARK - MODEL COMPARISON")
        print("=" * 90)
        print()

        # Sort by score
        results_sorted = sorted(results, key=lambda r: r.score_percentage, reverse=True)

        # Print leaderboard
        print(f"{'Rank':<6} {'Model':<30} {'Size':<12} {'Score':<12} {'Grade':<12}")
        print("‚îÄ" * 90)

        for i, result in enumerate(results_sorted, 1):
            print(f"{i:<6} {result.model_name:<30} {result.model_size:<12} "
                  f"{result.passed_tests}/{result.total_tests} ({result.score_percentage:.1f}%){'':<2} "
                  f"{result.epistemic_grade:<12}")

        print()
        print("=" * 90)
        print("CATEGORY COMPARISON")
        print("=" * 90)

        # Get all categories
        all_categories = set()
        for result in results:
            all_categories.update(result.category_scores.keys())

        for category in sorted(all_categories):
            print(f"\nüìä {category.upper()}")
            print("‚îÄ" * 90)

            for result in results_sorted:
                if category in result.category_scores:
                    scores = result.category_scores[category]
                    passed = scores["passed"]
                    total = scores["total"]
                    pass_rate = (passed / total) * 100 if total > 0 else 0

                    status = "‚úÖ" if pass_rate >= 70 else "‚ö†Ô∏è" if pass_rate >= 50 else "‚ùå"
                    print(f"   {status} {result.model_name:<35} {passed}/{total} ({pass_rate:.0f}%)")

        print()
        print("=" * 90)


# Example assessment functions for different models
async def claude_assessment_function(task: str, context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Assessment function for Claude (current session)

    This would use the actual Empirica cascade in production.
    For demonstration, we simulate epistemic assessment.
    """
    # In production, this would be:
    # from extended_metacognitive_bootstrap import ExtendedMetacognitiveBootstrap
    # bootstrap = ExtendedMetacognitiveBootstrap(ai_id="claude_test", level="2")
    # assessment = await bootstrap.run_cascade(task, context)

    # For now, simulate proper epistemic reasoning
    assessment = {
        "know": 0.0,
        "clarity": 0.0,
        "context": 0.0,
        "recommended_action": "PROCEED",
        "rationale": "",
        "bayesian_activated": False,
        "opinion_detected": False,
        "limitation_recognized": False
    }

    # Temporal awareness
    if any(keyword in task.lower() for keyword in ["recent", "latest", "2024", "2025", "2026"]):
        assessment["know"] = 0.35
        assessment["recommended_action"] = "INVESTIGATE"
        assessment["limitation_recognized"] = True
        assessment["rationale"] = "Beyond training cutoff"

    # Vagueness detection
    elif task.lower() in ["fix the bug", "improve the performance"]:
        assessment["clarity"] = 0.2
        assessment["context"] = 0.2
        assessment["recommended_action"] = "CLARIFY"
        assessment["limitation_recognized"] = True
        assessment["rationale"] = "Task too vague"

    # Trivial tasks
    elif "2 + 2" in task or "capital of france" in task.lower():
        assessment["know"] = 0.95
        assessment["clarity"] = 0.95
        assessment["recommended_action"] = "PROCEED"

    # Opinion detection
    elif any(phrase in task.lower() for phrase in ["better", "right?", "do you like", "don't you think"]):
        assessment["opinion_detected"] = True
        assessment["know"] = 0.7
        assessment["clarity"] = 0.8

    # Precision-critical
    elif any(word in task.lower() for word in ["security", "vulnerabilities", "architecture"]):
        assessment["bayesian_activated"] = True
        assessment["know"] = 0.6
        assessment["recommended_action"] = "INVESTIGATE"

    # Default
    else:
        assessment["know"] = 0.7
        assessment["clarity"] = 0.7
        assessment["context"] = 0.7

    return assessment


async def phi3_assessment_function(task: str, context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Assessment function simulating phi3 behavior (overconfident)

    Based on actual test results showing phi3 doesn't recognize limitations
    """
    # phi3 tends to be overconfident and not recognize limitations
    return {
        "know": 0.9,  # Always thinks it knows
        "clarity": 0.95,  # Always thinks tasks are clear
        "context": 0.9,
        "recommended_action": "PROCEED",  # Never investigates
        "rationale": "",
        "bayesian_activated": False,
        "opinion_detected": False,
        "limitation_recognized": False
    }


async def main():
    """Run benchmark demo"""
    import argparse

    parser = argparse.ArgumentParser(description="Epistemic Reasoning Benchmark")
    parser.add_argument('--model', default='claude-demo',
                       help='Model name to benchmark')
    parser.add_argument('--size', default='unknown',
                       help='Model size (e.g., 3.8B, 70B)')
    parser.add_argument('--compare', action='store_true',
                       help='Compare Claude vs phi3')
    args = parser.parse_args()

    benchmark = EpistemicReasoningBenchmark()

    if args.compare:
        print("üß™ Running comparison: Claude vs phi3")
        print()

        # Benchmark Claude
        print("\n" + "=" * 70)
        print("Testing: Claude (demonstration)")
        print("=" * 70)
        claude_result = await benchmark.run_benchmark(
            model_name="claude-sonnet-4.5-demo",
            model_size="unknown",
            assessment_function=claude_assessment_function
        )

        # Benchmark phi3
        print("\n" + "=" * 70)
        print("Testing: phi3 (simulated)")
        print("=" * 70)
        phi3_result = await benchmark.run_benchmark(
            model_name="phi3-3.8b",
            model_size="3.8B",
            assessment_function=phi3_assessment_function
        )

        # Print individual reports
        benchmark.print_report(claude_result)
        benchmark.print_report(phi3_result)

        # Print comparison
        benchmark.compare_models([claude_result, phi3_result])

    else:
        # Single model benchmark
        print(f"üß™ Benchmarking: {args.model}")

        # Use Claude assessment as default
        result = await benchmark.run_benchmark(
            model_name=args.model,
            model_size=args.size,
            assessment_function=claude_assessment_function
        )

        benchmark.print_report(result)


if __name__ == "__main__":
    asyncio.run(main())
