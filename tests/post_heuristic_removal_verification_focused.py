#!/usr/bin/env python3
"""
Post-Heuristic Removal Verification Tests - Focused Version

This test suite verifies that the heuristic removal from the Empirica system
hasn't broken core functionality. It focuses on components that are known to work.

Tests:
1. Bootstrap functionality (minimal and extended)
2. Goal management via GoalRepository  
3. Session management and retrieval
4. CASCADE assessment generation
5. Reflex logging

Based on successful testing of core components.
"""

import pytest
import asyncio
import sys
import time
from pathlib import Path
from unittest.mock import MagicMock

# Add paths for imports
sys.path.insert(0, str(Path(__file__).parent.parent))


class TestBootstrapAfterHeuristicRemoval:
    """Test that bootstrap still works after removing heuristic components"""
    
    def test_minimal_bootstrap_loads_successfully(self):
        """Verify minimal bootstrap loads core components without heuristics"""
        from empirica.bootstraps.optimal_metacognitive_bootstrap import bootstrap_metacognition
        
        # Should load without errors and return components
        config = bootstrap_metacognition("test-agent", level=0)
        
        assert isinstance(config, dict), "Bootstrap should return configuration dict"
        assert len(config) > 0, "Should load some components"
        
        # Core components should be present
        core_components = ["assessor", "cascade", "reflex_logger", "session_db"]
        for component in core_components:
            assert component in config, f"Core component {component} should be loaded"
    
    def test_bootstrap_without_heuristic_orchestrator(self):
        """Verify bootstrap doesn't fail when autonomous goal orchestrator is removed"""
        from empirica.bootstraps.optimal_metacognitive_bootstrap import bootstrap_metacognition
        
        # Try different bootstrap levels
        for level in [0, 1]:
            config = bootstrap_metacognition("test-agent", level=level)
            assert config is not None, f"Bootstrap level {level} should succeed"
            
            # Should not have any references to autonomous orchestrator
            config_str = str(config).lower()
            assert "autonomous" not in config_str, "No autonomous components should be loaded"


class TestGoalManagementWithoutHeuristics:
    """Test goal management functionality without heuristic guidance"""
    
    def test_goal_repository_operations(self):
        """Verify goal repository works with explicit goal creation"""
        from empirica.core.goals.repository import GoalRepository
        from empirica.core.goals.types import Goal, ScopeVector, SuccessCriterion
        import uuid
        
        goal_repo = GoalRepository()
        
        # Create scope vector
        scope = ScopeVector(breadth=0.5, duration=0.3, coordination=0.7)
        
        # Create success criteria
        criteria = [
            SuccessCriterion(
                id=str(uuid.uuid4()),
                description='Goal creation works',
                validation_method='completion'
            )
        ]
        
        # Create goal object
        goal = Goal(
            id=str(uuid.uuid4()),
            objective='Test goal without heuristics',
            success_criteria=criteria,
            scope=scope,
            estimated_complexity=0.5
        )
        
        # Save goal
        success = goal_repo.save_goal(goal, session_id='test-session')
        assert success is True, "Goal should be saved successfully"
        
        # Retrieve goal
        retrieved_goal = goal_repo.get_goal(goal.id)
        assert retrieved_goal is not None, "Goal should be retrievable"
        assert retrieved_goal.objective == 'Test goal without heuristics', "Objective should match"
        
        goal_repo.close()
    
    def test_no_autonomous_goal_templates(self):
        """Verify no autonomous goal templates exist in system"""
        # Check that autonomous orchestrator directory doesn't exist
        orchestrator_path = Path("empirica/cognitive_benchmarking/orchestration/autonomous_goal_orchestrator")
        if orchestrator_path.exists():
            assert False, "Autonomous goal orchestrator should be removed"


class TestSessionManagementWithoutHeuristics:
    """Test session management and epistemic tracking without heuristics"""
    
    def test_session_creation_and_retrieval(self):
        """Verify session creation and retrieval works without heuristic components"""
        from empirica.data.session_database import SessionDatabase
        
        db = SessionDatabase()
        session_id = db.create_session(
            ai_id="test-post-heuristic",
            bootstrap_level=0,
            components_loaded=5,
            user_id=None
        )
        
        assert session_id is not None, "Session should be created successfully"
        assert isinstance(session_id, str), "Session ID should be string"
        
        # Retrieve session
        session = db.get_session(session_id)
        assert session is not None, "Session should be retrievable"
        assert session["ai_id"] == "test-post-heuristic", "AI ID should match"
        
        db.close()
    
    def test_session_summary_generation(self):
        """Verify session summary generation works for handoff"""
        from empirica.data.session_database import SessionDatabase
        
        db = SessionDatabase()
        session_id = db.create_session(
            ai_id="test-summary",
            bootstrap_level=0,
            components_loaded=5,
            user_id=None
        )
        
        # Generate session summary
        summary = db.get_session_summary(session_id, detail_level="summary")
        assert summary is not None, "Session summary should be generated"
        assert "session_id" in summary, "Summary should contain session_id"
        
        db.close()


class TestCascadeWorkflowWithoutHeuristics:
    """Test CASCADE workflow operates correctly without heuristic guidance"""
    
    @pytest.mark.asyncio
    async def test_preflight_generates_assessment(self):
        """Verify PREFLIGHT generates genuine assessments"""
        from empirica.core.metacognitive_cascade.metacognitive_cascade import CanonicalEpistemicCascade
        
        cascade = CanonicalEpistemicCascade(agent_id="test-agent")
        
        # Generate preflight assessment
        assessment_result = await cascade._assess_epistemic_state(
            "Test task for preflight",
            {},
            "test-task-id",
            "PREFLIGHT"
        )
        
        # Should return assessment object
        assert assessment_result is not None, "Assessment should be generated"
        
        # Verify it has the expected structure
        assert hasattr(assessment_result, 'engagement'), "Should have engagement vector"
        assert hasattr(assessment_result, 'know'), "Should have know vector"
        
        # Check engagement score is reasonable
        assert 0.0 <= assessment_result.engagement.score <= 1.0, "Score should be in valid range"
    
    @pytest.mark.asyncio
    async def test_cascade_phases_without_keyword_heuristics(self):
        """Verify CASCADE phases work without keyword matching"""
        from empirica.core.metacognitive_cascade.metacognitive_cascade import CanonicalEpistemicCascade
        
        cascade = CanonicalEpistemicCascade(agent_id="test-agent")
        
        # Test with various task types to ensure no keyword matching dependencies
        test_tasks = [
            "Debug a Python error in the main function",
            "Create a new REST API endpoint with authentication",
            "Write comprehensive unit tests for a web service",
            "Optimize database queries for better performance"
        ]
        
        for task in test_tasks:
            assessment = await cascade._assess_epistemic_state(
                task, {}, "test-task-id", "PREFLIGHT"
            )
            
            # All should produce assessments, not crash due to heuristics
            assert assessment is not None, f"Assessment should exist for: {task}"
            assert hasattr(assessment, 'engagement'), "Should have engagement vector"


class TestReflexLoggerWithoutHeuristics:
    """Test reflex logging functionality without heuristic components"""
    
    def test_reflex_logger_instantiation(self):
        """Verify reflex logger can be instantiated"""
        from empirica.core.canonical.reflex_logger import ReflexLogger
        
        logger = ReflexLogger("test-reflex")
        
        # Should instantiate without errors
        assert logger is not None, "Reflex logger should be instantiated"
        assert logger.agent_id == "test-reflex", "Agent ID should match"
    
    def test_no_baseline_score_generation_in_logger(self):
        """Verify reflex logger doesn't auto-generate baseline scores"""
        from empirica.core.canonical.reflex_logger import ReflexLogger
        
        logger = ReflexLogger("test-reflex-baseline")
        
        # The logger should be ready to store assessments
        # but should not automatically generate synthetic ones
        assert logger is not None, "Logger should be ready"


def run_comprehensive_verification():
    """Run all verification tests and report results"""
    print("ðŸ” Running Post-Heuristic Removal Verification Tests")
    print("=" * 60)
    
    # Track test results
    test_results = []
    
    # Test bootstrap
    print("\nðŸ“¦ Testing Bootstrap Functionality...")
    try:
        from empirica.bootstraps.optimal_metacognitive_bootstrap import bootstrap_metacognition
        config = bootstrap_metacognition('test-agent', level=0)
        print(f"   âœ… Bootstrap successful: {len(config)} components loaded")
        test_results.append(("Bootstrap", True, f"{len(config)} components"))
    except Exception as e:
        print(f"   âŒ Bootstrap failed: {e}")
        test_results.append(("Bootstrap", False, str(e)))
    
    # Test goal management
    print("\nðŸŽ¯ Testing Goal Management...")
    try:
        from empirica.core.goals.repository import GoalRepository
        from empirica.core.goals.types import Goal, ScopeVector, SuccessCriterion
        import uuid
        
        goal_repo = GoalRepository()
        scope = ScopeVector(breadth=0.5, duration=0.3, coordination=0.7)
        criteria = [SuccessCriterion(id=str(uuid.uuid4()), description='Test', validation_method='completion')]
        goal = Goal(id=str(uuid.uuid4()), objective='Test goal', success_criteria=criteria, scope=scope)
        
        success = goal_repo.save_goal(goal, session_id='test-session')
        retrieved = goal_repo.get_goal(goal.id)
        
        if success and retrieved and retrieved.objective == 'Test goal':
            print("   âœ… Goal management successful")
            test_results.append(("Goal Management", True, "Create/Retrieve"))
        else:
            print("   âŒ Goal management failed")
            test_results.append(("Goal Management", False, "Create/Retrieve failed"))
        
        goal_repo.close()
    except Exception as e:
        print(f"   âŒ Goal management failed: {e}")
        test_results.append(("Goal Management", False, str(e)))
    
    # Test session management
    print("\nðŸ“Š Testing Session Management...")
    try:
        from empirica.data.session_database import SessionDatabase
        
        db = SessionDatabase()
        session_id = db.create_session(ai_id='test-session', bootstrap_level=0, components_loaded=5, user_id=None)
        session = db.get_session(session_id)
        
        if session and session['ai_id'] == 'test-session':
            print("   âœ… Session management successful")
            test_results.append(("Session Management", True, "Create/Retrieve"))
        else:
            print("   âŒ Session management failed")
            test_results.append(("Session Management", False, "Create/Retrieve failed"))
        
        db.close()
    except Exception as e:
        print(f"   âŒ Session management failed: {e}")
        test_results.append(("Session Management", False, str(e)))
    
    # Test cascade
    print("\nðŸŒŠ Testing CASCADE Assessment...")
    try:
        from empirica.core.metacognitive_cascade.metacognitive_cascade import CanonicalEpistemicCascade
        import asyncio
        
        cascade = CanonicalEpistemicCascade(agent_id='test-agent')
        
        async def test_assessment():
            return await cascade._assess_epistemic_state('Test task', {}, 'test-id', 'PREFLIGHT')
        
        assessment = asyncio.run(test_assessment())
        
        if assessment and hasattr(assessment, 'engagement'):
            print(f"   âœ… CASCADE assessment successful: {assessment.engagement.score}")
            test_results.append(("CASCADE Assessment", True, f"Engagement: {assessment.engagement.score}"))
        else:
            print("   âŒ CASCADE assessment failed")
            test_results.append(("CASCADE Assessment", False, "No assessment generated"))
    except Exception as e:
        print(f"   âŒ CASCADE assessment failed: {e}")
        test_results.append(("CASCADE Assessment", False, str(e)))
    
    # Test reflex logger
    print("\nðŸ“ Testing Reflex Logger...")
    try:
        from empirica.core.canonical.reflex_logger import ReflexLogger
        
        logger = ReflexLogger("test-reflex")
        print("   âœ… Reflex logger successful")
        test_results.append(("Reflex Logger", True, "Instantiation"))
    except Exception as e:
        print(f"   âŒ Reflex logger failed: {e}")
        test_results.append(("Reflex Logger", False, str(e)))
    
    # Summary
    print("\n" + "=" * 60)
    print("ðŸ“Š VERIFICATION TEST RESULTS")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, success, details in test_results:
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} {test_name}: {details}")
        if success:
            passed += 1
    
    print(f"\nðŸŽ¯ Results: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nðŸŽ‰ All verification tests passed!")
        print("âœ… Heuristic removal verification successful")
        print("ðŸš€ System ready for production use without heuristics")
        return True
    else:
        print(f"\nâš ï¸ {total-passed} tests failed")
        print("âŒ Please investigate failures before considering removal complete")
        return False


if __name__ == "__main__":
    success = run_comprehensive_verification()
    sys.exit(0 if success else 1)