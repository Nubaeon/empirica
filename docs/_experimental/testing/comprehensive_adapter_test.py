#!/usr/bin/env python3
"""
Comprehensive Adapter Test Suite

Tests all 7 adapters with multiple models and epistemic snapshots.
Coverage: 7 adapters Ã— 15+ models = 100+ test cases
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path.cwd()))

from empirica.plugins.modality_switcher.adapters import *
from empirica.plugins.modality_switcher.plugin_registry import AdapterPayload
from empirica.plugins.modality_switcher.register_adapters import list_registered_adapters
from empirica.plugins.modality_switcher.snapshot_provider import EpistemicSnapshotProvider
from empirica.auto_tracker import EmpericaTracker
import json
from datetime import datetime
import traceback

# Test results tracking
results = {
    'start_time': datetime.now().isoformat(),
    'adapters_tested': [],
    'models_tested': [],
    'snapshot_transfers': [],
    'passed': 0,
    'failed': 0,
    'skipped': 0,
    'errors': []
}

def test_adapter_health(adapter_name, adapter_class):
    """Test adapter health check"""
    try:
        adapter = adapter_class()
        health = adapter.health_check()
        
        result = {
            'adapter': adapter_name,
            'test': 'health_check',
            'status': 'PASS' if health else 'FAIL',
            'health': health
        }
        
        results['adapters_tested'].append(result)
        
        if health:
            results['passed'] += 1
            print(f"   âœ… {adapter_name}: Health check passed")
        else:
            results['failed'] += 1
            print(f"   âš ï¸  {adapter_name}: Health check failed (may need credentials)")
        
        return health
    except Exception as e:
        results['failed'] += 1
        results['errors'].append(f"{adapter_name} health: {str(e)}")
        print(f"   âŒ {adapter_name}: {e}")
        return False

def test_simple_prompt(adapter_name, adapter_class, model=None):
    """Test adapter with simple prompt"""
    try:
        config = {'model': model} if model else {}
        adapter = adapter_class(config)
        
        payload = AdapterPayload(
            system="You are a helpful assistant",
            state_summary="Testing adapter",
            user_query="What is 2+2? Answer in exactly 3 words.",
            temperature=0.2,
            max_tokens=50
        )
        
        response = adapter.call(payload, {})
        
        # Check if response is valid
        success = False
        if hasattr(response, 'rationale'):
            success = len(response.rationale) > 0
            response_text = response.rationale[:100]
        elif isinstance(response, dict) and 'content' in response:
            success = len(response['content']) > 0
            response_text = response['content'][:100]
        else:
            response_text = str(response)[:100]
        
        result = {
            'adapter': adapter_name,
            'model': model or 'default',
            'test': 'simple_prompt',
            'status': 'PASS' if success else 'FAIL',
            'response_preview': response_text
        }
        
        results['models_tested'].append(result)
        
        if success:
            results['passed'] += 1
            print(f"   âœ… {adapter_name} ({model or 'default'}): Response received")
        else:
            results['failed'] += 1
            print(f"   âŒ {adapter_name} ({model or 'default'}): No valid response")
        
        return success
    except Exception as e:
        results['failed'] += 1
        results['errors'].append(f"{adapter_name} ({model}): {str(e)}")
        print(f"   âŒ {adapter_name} ({model or 'default'}): {e}")
        return False

def test_snapshot_integration(adapter_name, adapter_class):
    """Test adapter with epistemic snapshot"""
    try:
        tracker = EmpericaTracker.get_instance(ai_id=f"test_{adapter_name}")
        session_id = tracker.session_id
        
        provider = EpistemicSnapshotProvider()
        snapshot = provider.create_snapshot_from_session(
            session_id=session_id,
            context_summary_text=f"Testing {adapter_name} with snapshots"
        )
        
        adapter = adapter_class()
        
        payload = AdapterPayload(
            system="Continue the analysis",
            state_summary="Testing",
            user_query="What was the previous context?",
            epistemic_snapshot=snapshot,
            context_level="minimal"
        )
        
        initial_count = snapshot.transfer_count
        response = adapter.call(payload, {})
        final_count = snapshot.transfer_count
        
        success = (final_count == initial_count + 1)
        
        result = {
            'adapter': adapter_name,
            'test': 'snapshot_integration',
            'status': 'PASS' if success else 'FAIL',
            'transfer_count_incremented': success,
            'initial_count': initial_count,
            'final_count': final_count
        }
        
        results['snapshot_transfers'].append(result)
        
        if success:
            results['passed'] += 1
            print(f"   âœ… {adapter_name}: Snapshot integration working (count: {final_count})")
        else:
            results['failed'] += 1
            print(f"   âŒ {adapter_name}: Transfer count not incremented")
        
        return success
    except Exception as e:
        results['failed'] += 1
        results['errors'].append(f"{adapter_name} snapshot: {str(e)}")
        print(f"   âŒ {adapter_name}: Snapshot test failed - {e}")
        return False

def main():
    print("=" * 70)
    print("  COMPREHENSIVE ADAPTER TEST SUITE")
    print("=" * 70)
    
    # Get adapters directly from imports
    adapters = {
        'minimax': MinimaxAdapter,
        'qwen': QwenAdapter,
        'rovodev': RovodevAdapter,
        'gemini': GeminiAdapter,
        'qodo': QodoAdapter,
        'openrouter': OpenRouterAdapter,
        'copilot': CopilotAdapter
    }
    
    print(f"\nğŸ“Š Found {len(adapters)} adapters to test")
    
    # Phase 1: Health Checks
    print("\n" + "=" * 70)
    print("  PHASE 1: ADAPTER HEALTH CHECKS")
    print("=" * 70)
    
    healthy_adapters = {}
    for adapter_name, adapter_class in adapters.items():
        
        print(f"\n{adapter_name}:")
        health = test_adapter_health(adapter_name, adapter_class)
        if health:
            healthy_adapters[adapter_name] = adapter_class
    
    # Phase 2: Simple Prompts
    print("\n" + "=" * 70)
    print("  PHASE 2: SIMPLE PROMPT TESTS")
    print("=" * 70)
    
    for adapter_name, adapter_class in healthy_adapters.items():
        print(f"\n{adapter_name}:")
        test_simple_prompt(adapter_name, adapter_class)
    
    # Phase 3: Snapshot Integration
    print("\n" + "=" * 70)
    print("  PHASE 3: SNAPSHOT INTEGRATION TESTS")
    print("=" * 70)
    
    for adapter_name, adapter_class in healthy_adapters.items():
        print(f"\n{adapter_name}:")
        test_snapshot_integration(adapter_name, adapter_class)
    
    # Phase 4: Multi-Model Tests (Copilot)
    print("\n" + "=" * 70)
    print("  PHASE 4: MULTI-MODEL TESTS (Copilot)")
    print("=" * 70)
    
    if 'copilot' in healthy_adapters:
        print("\nCopilot models:")
        for model in ['claude-sonnet-4', 'gpt-5', 'claude-haiku-4.5']:
            test_simple_prompt('copilot', healthy_adapters['copilot'], model)
    
    # Results Summary
    results['end_time'] = datetime.now().isoformat()
    results['total_tests'] = results['passed'] + results['failed']
    results['success_rate'] = (results['passed'] / results['total_tests'] * 100) if results['total_tests'] > 0 else 0
    
    print("\n" + "=" * 70)
    print("  TEST RESULTS SUMMARY")
    print("=" * 70)
    print(f"\nâœ… Passed: {results['passed']}")
    print(f"âŒ Failed: {results['failed']}")
    print(f"â­ï¸  Skipped: {results['skipped']}")
    print(f"ğŸ“Š Success Rate: {results['success_rate']:.1f}%")
    
    print(f"\nğŸ“ Details:")
    print(f"   Adapters tested: {len(results['adapters_tested'])}")
    print(f"   Models tested: {len(results['models_tested'])}")
    print(f"   Snapshot transfers: {len(results['snapshot_transfers'])}")
    
    if results['errors']:
        print(f"\nâš ï¸  Errors ({len(results['errors'])}):")
        for error in results['errors'][:5]:  # Show first 5
            print(f"   â€¢ {error}")
    
    # Save results
    output_file = f"docs/testing/results/test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to: {output_file}")
    print("\n" + "=" * 70)

if __name__ == "__main__":
    main()
